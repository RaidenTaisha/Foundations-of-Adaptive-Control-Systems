{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a6721cf",
   "metadata": {},
   "source": [
    "# ФИО выполнившего работу студента, номер группы и номер зачетной книжки\n",
    "---\n",
    "ФИО: Мяделец Андрей Алексеевич;  \n",
    "Группа: КИ19-07б;  \n",
    "Номер зачетной книжки: 031941203."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1.2 - Линейный классификатор (Linear classifier)\n",
    "\n",
    "В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n",
    "Тот класс, у которого эта сумма больше, и является предсказанием модели.\n",
    "\n",
    "В этом задании вы:\n",
    "- потренируетесь считать градиенты различных многомерных функций\n",
    "- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n",
    "- реализуете процесс тренировки линейного классификатора\n",
    "- подберете параметры тренировки на практике\n",
    "\n",
    "На всякий случай, еще раз ссылка на туториал по numpy:  \n",
    "http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_gradient\n",
    "from metrics import multiclass_accuracy \n",
    "import linear_classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, первым делом загружаем данные\n",
    "\n",
    "Мы будем использовать все тот же SVHN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_linear_classifier(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "\n",
    "    # Вычтем среднее арифметическое\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    # Добавим канал с единицами как постоянное смещение\n",
    "    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n",
    "    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n",
    "    return train_flat_with_ones, test_flat_with_ones\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)\n",
    "train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n",
    "# Разделим train на train и val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Играемся с градиентами!\n",
    "\n",
    "В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n",
    "\n",
    "Все функции, в которых мы будем вычислять градиенты, будут написаны по одной и той же схеме.  \n",
    "Они будут получать на вход точку, где нужно вычислить значение и градиент функции, а на выходе будут выдавать кортеж (tuple) из двух значений - собственно значения функции в этой точке (всегда одно число) и аналитического значения градиента в той же точке (той же размерности, что и вход).\n",
    "```\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Computes function and analytic gradient at x\n",
    "    \n",
    "    x: np array of float, input to the function\n",
    "    \n",
    "    Returns:\n",
    "    value: float, value of the function \n",
    "    grad: np array of float, same shape as x\n",
    "    \"\"\"\n",
    "    ...\n",
    "    \n",
    "    return value, grad\n",
    "```\n",
    "\n",
    "Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n",
    "\n",
    "Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в функции `check_gradient` в `gradient_check.py`. Эта функция будет принимать на вход функции формата, заданного выше, использовать значение `value` для вычисления численного градиента и сравнит его с аналитическим - они должны сходиться.\n",
    "\n",
    "Напишите часть функции, которая вычисляет градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n",
    "\n",
    "Все функции приведенные в следующей клетке должны проходить gradient check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def square(x):\n",
    "    return float(x*x), 2*x\n",
    "\n",
    "check_gradient(square, np.array([3.0]))\n",
    "\n",
    "def array_sum(x):\n",
    "    assert x.shape == (2,), x.shape\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_sum, np.array([3.0, 2.0]))\n",
    "\n",
    "def array_2d_sum(x):\n",
    "    assert x.shape == (2,2)\n",
    "    return np.sum(x), np.ones_like(x)\n",
    "\n",
    "check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Начинаем писать свои функции, считающие аналитический градиент\n",
    "\n",
    "Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n",
    "\n",
    "**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n",
    "\n",
    "К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n",
    "```\n",
    "predictions -= np.max(predictions)\n",
    "```\n",
    "(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализовали softmax и cross-entropy для единичной выборки\n",
    "probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n",
    "\n",
    "# Убедились, что с большими числами также работает\n",
    "probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n",
    "assert np.isclose(probs[0], 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n",
    "В общем виде cross-entropy определена следующим образом:\n",
    "![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n",
    "\n",
    "где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n",
    "В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n",
    "\n",
    "Это позволяет реализовать функцию проще!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.006760443547122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n",
    "linear_classifer.cross_entropy_loss(probs, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После того как мы реализовали сами функции, мы можем реализовать градиент.\n",
    "\n",
    "Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n",
    "\n",
    "Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Реализовали функцию, объединяющую softmax и cross entropy и вычисляющую градиент\n",
    "loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n",
    "\n",
    "Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n",
    "\n",
    "Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n",
    "\n",
    "Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Тест batch_size = 1\n",
    "num_classes = 4\n",
    "batch_size = 1\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int32)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Тест batch_size = 3\n",
    "num_classes = 4\n",
    "batch_size = 3\n",
    "predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float64)\n",
    "target_index = np.random.randint(0, num_classes, size=(batch_size, 1)).astype(np.int32)\n",
    "check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n",
    "\n",
    "# Снова убедимся, что нормирование работает для больших чисел в каждом батче.\n",
    "probs = linear_classifer.softmax(np.array([[20,0,0], [1000, 0, 0]]))\n",
    "assert np.all(np.isclose(probs[:, 0], 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наконец, реализуем сам линейный классификатор!\n",
    "\n",
    "softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n",
    "\n",
    "Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n",
    "\n",
    "Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n",
    "\n",
    "`predictions = X * W`, где `*` - матричное умножение.\n",
    "\n",
    "Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n",
    "batch_size = 2\n",
    "num_classes = 2\n",
    "num_features = 3\n",
    "np.random.seed(42)\n",
    "W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float64)\n",
    "X = np.random.randint(-1, 3, size=(batch_size, num_features)).astype(np.float64)\n",
    "target_index = np.ones(batch_size, dtype=np.int32)\n",
    "loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n",
    "check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### И теперь регуляризация\n",
    "\n",
    "Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n",
    "\n",
    "Напомним, L2 regularization определяется как\n",
    "\n",
    "l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n",
    "\n",
    "Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Реализовали функцию l2_regularization, вычисляющую ошибку для L2 регуляризации\n",
    "linear_classifer.l2_regularization(W, 0.01)\n",
    "check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Градиенты в порядке, реализуем процесс тренировки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 690.883617\n",
      "Epoch 1, loss: 688.211508\n",
      "Epoch 2, loss: 685.841336\n",
      "Epoch 3, loss: 683.710981\n",
      "Epoch 4, loss: 681.806399\n",
      "Epoch 5, loss: 680.090137\n",
      "Epoch 6, loss: 678.596557\n",
      "Epoch 7, loss: 677.321790\n",
      "Epoch 8, loss: 676.472948\n",
      "Epoch 9, loss: 676.264172\n"
     ]
    }
   ],
   "source": [
    "# Реализовали функцию LinearSoftmaxClassifier.fit\n",
    "classifier = linear_classifer.LinearSoftmaxClassifier()\n",
    "loss_history = classifier.fit(train_X, train_y, epochs=10, learning_rate=1e-3, batch_size=300, reg=1e1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26923cca6d0>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Посмотрим историю изменения ошибки!\n",
    "plt.plot(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.48538951358516513\n",
      "Epoch 0, loss: 678.288581\n",
      "Epoch 1, loss: 675.760484\n",
      "Epoch 2, loss: 674.224831\n",
      "Epoch 3, loss: 673.279360\n",
      "Epoch 4, loss: 672.683281\n",
      "Epoch 5, loss: 672.292405\n",
      "Epoch 6, loss: 672.021068\n",
      "Epoch 7, loss: 671.818937\n",
      "Epoch 8, loss: 671.656759\n",
      "Epoch 9, loss: 671.517694\n",
      "Epoch 10, loss: 671.392106\n",
      "Epoch 11, loss: 671.274509\n",
      "Epoch 12, loss: 671.161785\n",
      "Epoch 13, loss: 671.052161\n",
      "Epoch 14, loss: 670.944624\n",
      "Epoch 15, loss: 670.838597\n",
      "Epoch 16, loss: 670.733747\n",
      "Epoch 17, loss: 670.629878\n",
      "Epoch 18, loss: 670.526877\n",
      "Epoch 19, loss: 670.424674\n",
      "Epoch 20, loss: 670.323226\n",
      "Epoch 21, loss: 670.222505\n",
      "Epoch 22, loss: 670.122493\n",
      "Epoch 23, loss: 670.023175\n",
      "Epoch 24, loss: 669.924542\n",
      "Epoch 25, loss: 669.826585\n",
      "Epoch 26, loss: 669.729296\n",
      "Epoch 27, loss: 669.632669\n",
      "Epoch 28, loss: 669.536697\n",
      "Epoch 29, loss: 669.441375\n",
      "Epoch 30, loss: 669.346696\n",
      "Epoch 31, loss: 669.252655\n",
      "Epoch 32, loss: 669.159247\n",
      "Epoch 33, loss: 669.066467\n",
      "Epoch 34, loss: 668.974308\n",
      "Epoch 35, loss: 668.882766\n",
      "Epoch 36, loss: 668.791836\n",
      "Epoch 37, loss: 668.701512\n",
      "Epoch 38, loss: 668.611790\n",
      "Epoch 39, loss: 668.522664\n",
      "Epoch 40, loss: 668.434131\n",
      "Epoch 41, loss: 668.346183\n",
      "Epoch 42, loss: 668.258818\n",
      "Epoch 43, loss: 668.172030\n",
      "Epoch 44, loss: 668.085815\n",
      "Epoch 45, loss: 668.000167\n",
      "Epoch 46, loss: 667.915082\n",
      "Epoch 47, loss: 667.830556\n",
      "Epoch 48, loss: 667.746584\n",
      "Epoch 49, loss: 667.663161\n",
      "Epoch 50, loss: 667.580284\n",
      "Epoch 51, loss: 667.497946\n",
      "Epoch 52, loss: 667.416146\n",
      "Epoch 53, loss: 667.334877\n",
      "Epoch 54, loss: 667.254135\n",
      "Epoch 55, loss: 667.173917\n",
      "Epoch 56, loss: 667.094218\n",
      "Epoch 57, loss: 667.015034\n",
      "Epoch 58, loss: 666.936361\n",
      "Epoch 59, loss: 666.858195\n",
      "Epoch 60, loss: 666.780531\n",
      "Epoch 61, loss: 666.703365\n",
      "Epoch 62, loss: 666.626695\n",
      "Epoch 63, loss: 666.550515\n",
      "Epoch 64, loss: 666.474822\n",
      "Epoch 65, loss: 666.399611\n",
      "Epoch 66, loss: 666.324880\n",
      "Epoch 67, loss: 666.250624\n",
      "Epoch 68, loss: 666.176839\n",
      "Epoch 69, loss: 666.103522\n",
      "Epoch 70, loss: 666.030670\n",
      "Epoch 71, loss: 665.958277\n",
      "Epoch 72, loss: 665.886341\n",
      "Epoch 73, loss: 665.814859\n",
      "Epoch 74, loss: 665.743826\n",
      "Epoch 75, loss: 665.673238\n",
      "Epoch 76, loss: 665.603094\n",
      "Epoch 77, loss: 665.533388\n",
      "Epoch 78, loss: 665.464118\n",
      "Epoch 79, loss: 665.395280\n",
      "Epoch 80, loss: 665.326871\n",
      "Epoch 81, loss: 665.258888\n",
      "Epoch 82, loss: 665.191327\n",
      "Epoch 83, loss: 665.124184\n",
      "Epoch 84, loss: 665.057457\n",
      "Epoch 85, loss: 664.991142\n",
      "Epoch 86, loss: 664.925237\n",
      "Epoch 87, loss: 664.859737\n",
      "Epoch 88, loss: 664.794641\n",
      "Epoch 89, loss: 664.729944\n",
      "Epoch 90, loss: 664.665643\n",
      "Epoch 91, loss: 664.601737\n",
      "Epoch 92, loss: 664.538221\n",
      "Epoch 93, loss: 664.475092\n",
      "Epoch 94, loss: 664.412348\n",
      "Epoch 95, loss: 664.349985\n",
      "Epoch 96, loss: 664.288002\n",
      "Epoch 97, loss: 664.226394\n",
      "Epoch 98, loss: 664.165159\n",
      "Epoch 99, loss: 664.104294\n",
      "Accuracy after training for 100 epochs:  0.5378402200821127\n"
     ]
    }
   ],
   "source": [
    "# TODO: Реализовать функцию вычисления точности и посмотреть точность обучения\n",
    "pred = classifier.predict(val_X)\n",
    "#print(pred)\n",
    "#print(val_y)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n",
    "# Теперь возьмем больше эпох, меньшую скорость обучения и посмотрим как изменится точность\n",
    "classifier.fit(train_X, train_y, epochs=100, learning_rate=1e-4, batch_size=300, reg=1e1)\n",
    "pred = classifier.predict(val_X)\n",
    "accuracy = multiclass_accuracy(pred, val_y)\n",
    "print(\"Accuracy after training for 100 epochs: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как и раньше, используем кросс-валидацию для подбора гиперпараметров.\n",
    "\n",
    "В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n",
    "\n",
    "Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n",
    "Добейтесь точности более чем **20%** на проверочных данных (validation data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4989509024607687, 0.5141254844993283, 0.5276490628266179, 0.5446725420479139, 0.5548072047111952, 0.5603666313261564, 0.5642572126636385, 0.5674454812118592, 0.5697897576809575]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 300\n",
    "\n",
    "learning_rates = [1e-3, 1e-4, 1e-5]\n",
    "reg_strengths = [1e-4, 1e-5, 1e-6]\n",
    "pred = []\n",
    "accuracy = []\n",
    "best_classifier = 0.0\n",
    "best_val_accuracy = 0.0\n",
    "for learning_rate in learning_rates:\n",
    "    for reg in reg_strengths:\n",
    "        classifier.fit(train_X, train_y, batch_size, learning_rate, reg, num_epochs)\n",
    "        # pred.append(classifier.predict(val_X))\n",
    "        # accuracy.append(multiclass_accuracy(pred, val_y))\n",
    "        if (best_val_accuracy < multiclass_accuracy(pred, val_y)):\n",
    "            best_val_accuracy = multiclass_accuracy(pred, val_y)\n",
    "            best_classifier = classifier\n",
    "print(accuracy)\n",
    "\n",
    "# TODO: используя набор данных для валидации найти лучшие гиперпараметры\n",
    "#print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Какой же точности мы добились на тестовых данных?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear softmax classifier test set accuracy: 0.515587\n"
     ]
    }
   ],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cb842bff3a72caef711a3b0fe09c73464562168ec735fc11a8f3b798485135ad"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
